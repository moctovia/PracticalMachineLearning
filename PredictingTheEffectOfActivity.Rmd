---
title: "Predicting Activity using Weight Lifting Exercise Dataset"
author: "Michelle O"
date: "March 11, 2016"
output: html_document
---

## Project Overview
This report will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did barbell lifts. The individuals were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal is to predict which of the 5 ways the barbell lifts were performed using measurements from the accelerometers. The 5 ways are represented by the "classe" variable within the training data. They have values "A", which is the correct way to lift and "B", "C", "D", "E", which are the incorrect way to lift.


## Weight Lifting Exercises Dataset Source
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har

## Exploratory data analysis
We load the training and testing data set into R. Both data sets have 160 variables. Histogram shows the distribution of classe.
```{r, echo= TRUE}
trainingData<-read.csv(file = "C:/Users/moctovia/OneDrive/Dev Training/Documents/COursera/Practical Machine Learning/CourseProject - Activity Data/pml-training.csv")

testingData<-read.csv(file = "C:/Users/moctovia/OneDrive/Dev Training/Documents/COursera/Practical Machine Learning/CourseProject - Activity Data/pml-testing.csv")

dim(trainingData) # 160 variables
dim(testingData) 
barplot(table(trainingData$classe), xlab="classe", ylab="quantity", main="Classe distribution in the training data")
```

## Preprocessing
Summary of the dataset shows that there invalid and empty strings. We replace these invalid values to NA in both test and train data sets
```{r, echo= TRUE}
# Replace any #DIV/0, "NA" and empty strings as NA
trainingData[trainingData==""]<-NA
trainingData[trainingData=="#DIV/0!"]<-NA
trainingData[trainingData=="NA"]<-NA

testingData[testingData==""]<-NA
testingData[testingData=="#DIV/0!"]<-NA
testingData[testingData=="NA"]<-NA
```
Next, we remove the first 7 variables, which are identifiers and timestamps, and are not measurements of the activity.
```{r, echo= TRUE}
drop<-c("X", "user_name","raw_timestamp_part_1", "raw_timestamp_part_2",
                                 "cvtd_timestamp","new_window","num_window")
trainingData1<-trainingData[,!names(trainingData)%in% drop]
testingData1<-testingData[,!names(testingData)%in% drop]

```

## Predictor Selection
We will only use variables that are related to the raw measurements of the sensors. These variables appear with the following name patterns in the dataset:
gyros_xxx_x|y|z, 
accel_xxx_x|y|z, 
total_accel_xxx, 
magnet_xxx_x|y|z, 
roll|pitch|yaw_xxx. 

I used only data with the above column names. We have filtered the number of predictors down to 52.
```{r, echo= TRUE}
predictors <- c(grep("^accel", names(trainingData1)), grep("^gyros", names(trainingData1)), grep("^magnet", names(trainingData1)), grep("^roll", names(trainingData1)), grep("^pitch", names(trainingData1)), grep("^yaw", names(trainingData1)), grep("^total", names(trainingData1)))
trainingData2 <- trainingData1[, c(predictors, 153)]
testingData2 <- testingData1[, c(predictors, 153)]
```

Variables with near zero variance will not be considered as predictors. All 52 selected have nzv far from zero.
```{r, echo= TRUE}
library(caret)
library(ggplot2)
library(randomForest)
nsv<-nearZeroVar(trainingData2, saveMetrics = TRUE, names = FALSE)
nsv
```



## Cross Validation
I split 75% of the trainingData2 data into a training data set and the remaining 25% into a validation data set. The models will be trained using the new training subset.

```{r, echo= TRUE}
inTrain<-createDataPartition(y = trainingData2$classe, p = 0.75, list = FALSE)
training<-trainingData2[inTrain,]
validation<-trainingData2[-inTrain,]
trControl = trainControl(method = "cv", number = 4, allowParallel =TRUE);

```
## Model Building
Set up parallel processing to improve speed.

```{r, echo= TRUE}
library(doParallel);
rCluster <- makePSOCKcluster(4);
registerDoParallel(rCluster);
```

I built models using random forest and gradient boosting machine methods. The 2 models are cross validated using the validation data set. 
```{r, echo= TRUE, cache=TRUE}
set.seed(125)
modFitRF<-train(classe ~ ., data=training, method = "rf", trControl = trControl, ntree = 250)
modFitGBM<-train(classe~ ., data=training, method="gbm",trControl= trControl, verbose= FALSE)

predRF<-predict(modFitRF,newdata = validation)
predGBM<-predict(modFitGBM,validation)
```

## Evaluate Error Rates
Confusion Matrices are used to evaluate the accuracy of the 2 models. 
Random Forest's Confusion Matrix:
```{r, echo= TRUE}
confusionMatrix(predRF, validation$classe)
```
Gradient Boosting Machine's Confusion Matrix:
```{r, echo= TRUE}
confusionMatrix(predGBM, validation$classe)
```

RF method does the best job of predicting Classe A with 0/1395 (0%) out of sample error, and worst at predicting Classe D with 18/804 (2.2%) out of sample errors. Overall, RF method has 31/4904 (0.6%) out of sample error rate across all classes. 

GBM method does the best job of predicting Classe A with 18/1413 (1.3%) out of sample errors, and worst at predicting Classe D with 52/856 (6.1%) out of sample errors. Overall, GBM method has 176/5081 (3.5%) out of sample error rate across all classes. 

Random Forest method yields a better accuracy (99%) than gradient boosting machine (96%). Based on these reasons, I will pick Random Forest to predict my testing data.

## Predicting Test Cases
I ran the testing dataset through the random forest model to predict the 20 test cases available in the test data. Results are as follows:
```{r, echo= TRUE}
predTestingRF<-predict(modFitRF,newdata = testingData2)
predTestingRF

```
